{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of EHB_Masking_Medical_Dataset_Other_Data_Cross_Validation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdT9BTrmT3ro",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQNUpL6YT32X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import set_random_seed\n",
        "\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from keras.layers import Lambda, Input, Dense, Dropout\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from keras.objectives import mse\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dropout, Dense\n",
        "from keras.regularizers import l1, l2\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from collections import defaultdict\n",
        "\n",
        "GLOBAL_SEED = 1\n",
        "LOCAL_SEED = 42\n",
        "\n",
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "%matplotlib inline\n",
        "%pylab inline\n",
        "rcParams['figure.figsize'] = [10, 8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VTGWwfObeoT",
        "colab_type": "text"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFaXts2Ycg-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define PATH to file\n",
        "path = '/DataSets/Selected/breast-cancer-wisconsin/wdbc.data'\n",
        "# Dataset will be generated with the prefix:\n",
        "dest = '/Dataset/wdbc'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfcE15dwhyaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "set_random_seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "random.seed(GLOBAL_SEED)\n",
        "import pandas as pd\n",
        "na_values = {'?', np.nan}\n",
        "df = pd.read_csv(path,\n",
        "                 sep=',',\n",
        "                 header=None,\n",
        "                 na_filter=True, \n",
        "                 verbose=False, \n",
        "                 skip_blank_lines=True, \n",
        "                 na_values=na_values,\n",
        "                 keep_default_na=False)\n",
        "print('Origin dataset:')                 \n",
        "print(df.head())\n",
        "# Drop N/A \n",
        "df.replace('U', np.nan, inplace=True)\n",
        "df.dropna(axis='rows', how='all', inplace=True)\n",
        "df = shuffle(df, random_state=GLOBAL_SEED)\n",
        "df.drop([0], axis=1, inplace=True)\n",
        "print(df.head())\n",
        "\n",
        "col_names = list(df)\n",
        "new_names = {}\n",
        "for i, name in enumerate(col_names):\n",
        "    new_names[name] = 'X' + str(i)\n",
        "df.rename(columns=new_names, inplace=True)\n",
        "df = df.reindex(sorted(df.columns), axis=1)\n",
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLSi3TB78DhR",
        "colab_type": "text"
      },
      "source": [
        "## Handling categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfL6yO7HZR7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For breast cancer\n",
        "# df['X9'] = df['X9'].astype('category')\n",
        "\n",
        "# For Pima Diabetes\n",
        "cat_cols = ['X0']\n",
        "df[cat_cols] = df[cat_cols].astype('category')\n",
        "\n",
        "colnums = len(df.columns)\n",
        "for i in df.columns:\n",
        "    try:\n",
        "        if df[i].dtype.name == 'object' or df[i].dtype.name == 'category':\n",
        "            df[i] = df[i].astype('category')\n",
        "        else:\n",
        "            df[i] = df[i].astype('float32')\n",
        "    except:\n",
        "        continue\n",
        "df.dropna(axis='rows', how='any', inplace=True)\n",
        "print(df.head())\n",
        "print(df.describe())\n",
        "print(df.info())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pv-qnQtJTuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_non_null = df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRRZgCPE2WJS",
        "colab_type": "text"
      },
      "source": [
        "# Make data become missing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJBNlVLoJloq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import resample\n",
        "# make 50% of the data becoming missing\n",
        "prob_missing = 0.5\n",
        "df_incomplete = df_non_null.copy()\n",
        "ix = [(row, col) for row in range(df_non_null.shape[0]) for col in range(df_non_null.shape[1])]\n",
        "L = resample(ix, n_samples = int(prob_missing*len(ix)), \n",
        "             random_state=LOCAL_SEED)\n",
        "for row, col in L:\n",
        "    df_incomplete.iat[row, col] = np.nan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDDT8QZXMZ9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_incomplete.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDlflMKPZSn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "missing_encoded = pd.get_dummies(df_incomplete)\n",
        "\n",
        "for col in df.columns:\n",
        "    missing_cols = missing_encoded.columns.str.startswith(str(col) + \"_\")\n",
        "    missing_encoded.loc[df_incomplete[col].isnull(), missing_cols] = np.nan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntnenzc17rDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "missing_encoded.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_7GuK_df_TZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 1000\n",
        "n_epochs = 100\n",
        "n_batch_size=1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dNiaTLkMxq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def masked_mae(X_true, X_pred, mask):\n",
        "    masked_diff = X_true[mask] - X_pred[mask]\n",
        "    return np.mean(np.abs(masked_diff))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "shurAQCSAYlV",
        "colab": {}
      },
      "source": [
        "def reverse_encoding(df_test_dummies):\n",
        "    names = list(df_test_dummies)\n",
        "    c_dict = {}\n",
        "    for n in names:\n",
        "        if '_' in n:\n",
        "            index = n.index('_')\n",
        "            c_dict[n[:index]] = [c for c in names if n[:index+1] in c]\n",
        "    values = []\n",
        "    for key, items in c_dict.items():\n",
        "        dummies = df_test_dummies[items]\n",
        "        d_names = list(dummies)\n",
        "        c_dict = {}\n",
        "        for n in d_names:\n",
        "            c_dict[n] = n[n.index('_')+1:]\n",
        "        dummies.rename(columns=c_dict, \n",
        "                    inplace=True)\n",
        "        df_test_dummies[key] = dummies.idxmax(axis=1)\n",
        "        df_test_dummies.drop(items, axis=1, inplace=True)\n",
        "    print(df_test_dummies.head())\n",
        "    return df_test_dummies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Eg6YJ1tvfP",
        "colab_type": "text"
      },
      "source": [
        "# AutoEncoder with Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j-GqCbvMptj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AutoEncoderDropout:\n",
        "    def __init__(self, \n",
        "                 n_dims,\n",
        "                 recurrent_weight=0.5,\n",
        "                 optimizer=\"adam\",\n",
        "                 dropout_probability=0.5,\n",
        "                 hidden_activation=\"relu\",\n",
        "                 output_activation=\"sigmoid\",\n",
        "                 init=\"glorot_normal\",\n",
        "                 l1_penalty=1e-3,\n",
        "                 l2_penalty=1e-3,\n",
        "                 hidden_size=hidden_size):\n",
        "        self.n_dims = n_dims\n",
        "        self.recurrent_weight = recurrent_weight\n",
        "        self.optimizer = optimizer\n",
        "        self.dropout_probability = dropout_probability\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.output_activation = output_activation\n",
        "        self.init = init\n",
        "        self.l1_penalty = l1_penalty\n",
        "        self.l2_penalty = l2_penalty\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def make_reconstruction_loss(self, n_features):\n",
        "        def reconstruction_loss(input_and_mask, y_pred):\n",
        "            X_values = input_and_mask[:, :n_features]\n",
        "            missing_mask = input_and_mask[:, n_features:]\n",
        "            observed_mask = 1 - missing_mask\n",
        "            X_values_observed = X_values * observed_mask\n",
        "            pred_observed = y_pred * observed_mask\n",
        "            return binary_crossentropy(y_true=X_values_observed, \n",
        "                                       y_pred=pred_observed)\n",
        "        return reconstruction_loss\n",
        "\n",
        "    def _create_model(self):\n",
        "        latent_dim = int(np.ceil(self.n_dims*0.5))\n",
        "        inputs = Input(shape=(2*self.n_dims, ), \n",
        "                       name='encoder_input')\n",
        "        x = inputs\n",
        "        x = Dense(self.hidden_size, activation=self.hidden_activation,\n",
        "                  init=self.init,\n",
        "                  kernel_regularizer=l2(self.l2_penalty),\n",
        "                  bias_regularizer=l2(self.l2_penalty))(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        x = Dense(self.hidden_size//4, activation=self.hidden_activation,\n",
        "                  init=self.init,\n",
        "                  kernel_regularizer=l2(self.l2_penalty),\n",
        "                  bias_regularizer=l2(self.l2_penalty))(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        encoded = Dense(latent_dim, name='encoding')(x)\n",
        "        self.encoder = Model(inputs, encoded, name='encoder')\n",
        "        latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
        "        x = latent_inputs\n",
        "        x = Dense(self.hidden_size//4, activation=self.hidden_activation,\n",
        "                  init=self.init,\n",
        "                  kernel_regularizer=l2(self.l2_penalty),\n",
        "                  bias_regularizer=l2(self.l2_penalty))(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        x = Dense(self.hidden_size, activation=self.hidden_activation,\n",
        "                  init=self.init,\n",
        "                  kernel_regularizer=l2(self.l2_penalty),\n",
        "                  bias_regularizer=l2(self.l2_penalty))(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        outputs = Dense(self.n_dims, activation=self.output_activation,\n",
        "                        init=self.init,\n",
        "                        kernel_regularizer=l2(self.l2_penalty),\n",
        "                        bias_regularizer=l2(self.l2_penalty))(x)\n",
        "        self.decoder = Model(latent_inputs, \n",
        "                             outputs, \n",
        "                             name='decoder')\n",
        "        outputs = self.decoder(self.encoder(inputs))\n",
        "        self.model = Model(inputs, outputs, name='ae_mlp')\n",
        "        loss_function = self.make_reconstruction_loss(self.n_dims)\n",
        "        self.model.compile(optimizer=self.optimizer, \n",
        "                           loss=loss_function)\n",
        "\n",
        "    def fill(self, data, missing_mask):\n",
        "        data[missing_mask] = -1\n",
        "        return data\n",
        "\n",
        "    def _create_missing_mask(self, data):\n",
        "        if data.dtype != \"f\" and data.dtype != \"d\":\n",
        "            data = data.astype(float)\n",
        "        return np.isnan(data)\n",
        "\n",
        "    def _train_epoch(self, data, missing_mask, batch_size):\n",
        "        input_with_mask = np.hstack([data, missing_mask])\n",
        "        n_samples = len(input_with_mask)\n",
        "        n_batches = int(np.ceil(n_samples / batch_size))\n",
        "        indices = np.arange(n_samples)\n",
        "        np.random.shuffle(indices)\n",
        "        X_shuffled = input_with_mask[indices]\n",
        "        for batch_idx in range(n_batches):\n",
        "            batch_start = batch_idx * batch_size\n",
        "            batch_end = (batch_idx + 1) * batch_size\n",
        "            batch_data = X_shuffled[batch_start:batch_end, :]\n",
        "            self.model.train_on_batch(batch_data, batch_data)\n",
        "        return self.model.predict(input_with_mask)\n",
        "    \n",
        "    def predict(self, x_test_with_mask):\n",
        "        predict_stochastic = K.function([self.decoder.layers[0].input,\n",
        "                                        K.learning_phase()],\n",
        "                                        [self.decoder.layers[-1].output])\n",
        "        latent_input = self.encoder.predict(x_test_with_mask)\n",
        "    \n",
        "        outputs = np.array([np.array(predict_stochastic([latent_input, \n",
        "                                                            1])).reshape((x_test_with_mask.shape[0], \n",
        "                                                                        x_test_with_mask.shape[1]//2)) for _ in range(50)])\n",
        "        return np.mean(outputs, axis=0)  \n",
        "\n",
        "    def train(self, x_train, x_test, batch_size=256, train_epochs=100):\n",
        "        missing_mask = self._create_missing_mask(x_train)\n",
        "        x_train = self.fill(x_train, missing_mask)\n",
        "        x_test_missing_mask = self._create_missing_mask(x_test) \n",
        "        x_test = self.fill(x_test, x_test_missing_mask)\n",
        "\n",
        "        self._create_model()\n",
        "        \n",
        "        observed_mask = ~missing_mask\n",
        "        x_test_observed_mask = ~x_test_missing_mask\n",
        "        input_with_mask = np.hstack([x_train, missing_mask])\n",
        "        for epoch in range(train_epochs):\n",
        "            X_pred = self._train_epoch(x_train, missing_mask, batch_size)\n",
        "            x_test_with_mask = np.hstack([x_test, x_test_missing_mask])\n",
        "            X_test_pred = self.predict(x_test_with_mask)\n",
        "            observed_mae = masked_mae(X_true=x_train,\n",
        "                                    X_pred=X_pred,\n",
        "                                    mask=observed_mask)\n",
        "            test_observed_mae = masked_mae(X_true=x_test,\n",
        "                                           X_pred = X_test_pred,\n",
        "                                           mask=x_test_observed_mask)\n",
        "            if epoch % 50 == 0:\n",
        "                print(\"observed mae:\", observed_mae)\n",
        "                print(\"Test mae:\", test_observed_mae)\n",
        "            old_weight = (1.0 - self.recurrent_weight)\n",
        "            x_train[missing_mask] *= old_weight\n",
        "            x_test[x_test_missing_mask] *= old_weight\n",
        "            pred_missing = X_pred[missing_mask]\n",
        "            x_test_pred_missing = X_test_pred[x_test_missing_mask]\n",
        "            x_train[missing_mask] += self.recurrent_weight * pred_missing\n",
        "            x_test[x_test_missing_mask] += self.recurrent_weight * x_test_pred_missing\n",
        "        return x_train.copy(), x_test.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3BA-97jlmzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "seeds = [LOCAL_SEED+2, LOCAL_SEED+1, LOCAL_SEED+4, LOCAL_SEED+6, LOCAL_SEED+8]\n",
        "rmses = []\n",
        "cols = df_non_null.columns\n",
        "non_null_values = df_non_null.values.copy()\n",
        "for seed_number in seeds:\n",
        "    values = missing_encoded.values.copy()\n",
        "    train, test, comp_train, comp_test = train_test_split(values.copy(),\n",
        "                                                        non_null_values.copy(),\n",
        "                                                        test_size=0.1,\n",
        "                                                        random_state=seed_number)\n",
        "    df_test_complete = pd.DataFrame(columns=cols, \n",
        "                                    data=comp_test.copy())\n",
        "    scaler = MinMaxScaler().fit(train)\n",
        "    x_train = scaler.transform(train)\n",
        "    x_test = scaler.transform(test)\n",
        "    n_dims = x_train.shape[1]\n",
        "    aedropout = AutoEncoderDropout(n_dims=n_dims)\n",
        "    complete_encoded = aedropout.train(x_train.copy(), \n",
        "                                    x_test.copy(), \n",
        "                                    train_epochs=n_epochs,\n",
        "                                    batch_size=n_batch_size)\n",
        "    train_encoded, test_encoded = complete_encoded\n",
        "    missing_cols = list(missing_encoded)\n",
        "    inverse_test_encoded = scaler.inverse_transform(test_encoded)\n",
        "    df_test_dummies = pd.DataFrame(columns=missing_cols, \n",
        "                                   data=inverse_test_encoded)\n",
        "    df_test_dummies = reverse_encoding(df_test_dummies.copy())\n",
        "    df_test_dummies.drop(cat_cols, axis=1,\n",
        "                         inplace=True)\n",
        "    df_test_complete.drop(cat_cols, axis=1,\n",
        "                          inplace=True)\n",
        "    true_vals = df_test_complete.values.copy()\n",
        "    test_vals = df_test_dummies.values.copy()\n",
        "    scaler2 = MinMaxScaler().fit(true_vals)\n",
        "    scaled_true_vales = scaler2.transform(true_vals)\n",
        "    scaled_test_vales = scaler2.transform(test_vals)\n",
        "    rmse = math.sqrt(mean_squared_error(scaled_true_vales, \n",
        "                                        scaled_test_vales))\n",
        "    rmses.append(rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXOyIjf6n20u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.mean(rmses), np.std(rmses))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_a6aJX6aDbA",
        "colab_type": "text"
      },
      "source": [
        "# VAE MCD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eNCDt7r_aIxN",
        "colab": {}
      },
      "source": [
        "class VAEDropout:\n",
        "\n",
        "    def __init__(self, n_dims,\n",
        "                 recurrent_weight=0.5,\n",
        "                 optimizer=\"adam\",\n",
        "                 dropout_probability=0.5,\n",
        "                 hidden_activation=\"relu\",\n",
        "                 output_activation=\"sigmoid\",\n",
        "                 init=\"glorot_normal\",\n",
        "                 l2_penalty=1e-3,\n",
        "                 hidden_size=hidden_size):\n",
        "        self.n_dims = n_dims\n",
        "        self.recurrent_weight = recurrent_weight\n",
        "        self.optimizer = optimizer\n",
        "        self.dropout_probability = dropout_probability\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.output_activation = output_activation\n",
        "        self.init = init\n",
        "        self.l2_penalty = l2_penalty\n",
        "        self.hidden_size = hidden_size\n",
        "        self._create_model()\n",
        "\n",
        "    def _create_model(self):\n",
        "        latent_dim = int(np.ceil(self.n_dims*0.5))\n",
        "        inputs = Input(shape=(2*n_dims, ), name='encoder_input')\n",
        "        x = inputs\n",
        "        x = Dense(self.hidden_size, activation=self.hidden_activation,\n",
        "                  init=self.init,\n",
        "                  kernel_regularizer=l2(self.l2_penalty),\n",
        "                  bias_regularizer=l2(self.l2_penalty))(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        x = Dense(self.hidden_size//4, activation=self.hidden_activation,\n",
        "                  init=self.init,\n",
        "                  kernel_regularizer=l2(self.l2_penalty),\n",
        "                  bias_regularizer=l2(self.l2_penalty))(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "        z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "        self.encoder = Model(inputs, [z_mean, \n",
        "                                      z_log_var], name='encoder')\n",
        "        latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "        x = latent_inputs\n",
        "        x = Dense(self.hidden_size//4, activation=self.hidden_activation,\n",
        "                  init=self.init,\n",
        "                  kernel_regularizer=l2(self.l2_penalty),\n",
        "                  bias_regularizer=l2(self.l2_penalty))(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        x = Dense(self.hidden_size, activation=self.hidden_activation,\n",
        "                 init=self.init,\n",
        "                 kernel_regularizer=l2(self.l2_penalty),\n",
        "                  bias_regularizer=l2(self.l2_penalty))(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        outputs = Dense(self.n_dims, activation=self.output_activation,\n",
        "                        init=self.init,\n",
        "                       kernel_regularizer=l2(self.l2_penalty),\n",
        "                        bias_regularizer=l2(self.l2_penalty))(x)\n",
        "        self.decoder = Model(latent_inputs, \n",
        "                             outputs, \n",
        "                             name='decoder')\n",
        "        outputs = self.decoder(self.encoder(inputs)[0])\n",
        "        self.model = Model(inputs, outputs, \n",
        "                           name='vae_mlp')\n",
        "        reconstruction_loss = self.make_vae_reconstruction_loss(n_dims, \n",
        "                                                       z_mean, \n",
        "                                                       z_log_var)\n",
        "        self.model.compile(optimizer=self.optimizer, \n",
        "                           loss=reconstruction_loss)\n",
        "\n",
        "    def make_vae_reconstruction_loss(self, n_features, z_mean, z_log_var):\n",
        "        def reconstruction_loss(input_and_mask, y_pred):\n",
        "            X_values = input_and_mask[:, :n_features]\n",
        "        \n",
        "            missing_mask = input_and_mask[:, n_features:]\n",
        "            observed_mask = 1 - missing_mask\n",
        "            X_values_observed = X_values * observed_mask\n",
        "            pred_observed = y_pred * observed_mask\n",
        "            reconstruction_loss = binary_crossentropy(y_true=X_values_observed, \n",
        "                                        y_pred=pred_observed)\n",
        "            reconstruction_loss*=n_features\n",
        "            kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "            kl_loss = K.sum(kl_loss, axis=-1)\n",
        "            kl_loss *= -0.5\n",
        "            vae_loss = K.mean(reconstruction_loss + kl_loss)\t\n",
        "            return vae_loss\n",
        "        return reconstruction_loss\n",
        "\n",
        "    def fill(self, data, missing_mask):\n",
        "        data[missing_mask] = -1\n",
        "        return data\n",
        "\n",
        "    def _create_missing_mask(self, data):\n",
        "        if data.dtype != \"f\" and data.dtype != \"d\":\n",
        "            data = data.astype(float)\n",
        "\n",
        "        return np.isnan(data)\n",
        "\n",
        "    def predict(self, x_test_with_mask):\n",
        "        predict_stochastic = K.function([self.decoder.layers[0].input,\n",
        "                                        K.learning_phase()],\n",
        "                                        [self.decoder.layers[-1].output])\n",
        "        latent_input = self.encoder.predict(x_test_with_mask)\n",
        "    \n",
        "        outputs = np.array([np.array(predict_stochastic([latent_input, \n",
        "                                                            1])).reshape((x_test_with_mask.shape[0], \n",
        "                                                                        x_test_with_mask.shape[1]//2)) for _ in range(50)])\n",
        "        return np.mean(outputs, axis=0)  \n",
        "\n",
        "    def _train_epoch(self, data, missing_mask, batch_size):\n",
        "        input_with_mask = np.hstack([data, missing_mask])\n",
        "        n_samples = len(input_with_mask)\n",
        "        n_batches = int(np.ceil(n_samples / batch_size))\n",
        "        indices = np.arange(n_samples)\n",
        "        np.random.shuffle(indices)\n",
        "        X_shuffled = input_with_mask[indices]\n",
        "        for batch_idx in range(n_batches):\n",
        "            batch_start = batch_idx * batch_size\n",
        "            batch_end = (batch_idx + 1) * batch_size\n",
        "            batch_data = X_shuffled[batch_start:batch_end, :]\n",
        "            self.model.train_on_batch(batch_data, batch_data)\n",
        "        return self.model.predict(input_with_mask)\n",
        "\n",
        "    def train(self, x_train, x_test, batch_size=256, train_epochs=100):\n",
        "        missing_mask = self._create_missing_mask(x_train)\n",
        "        x_train = self.fill(x_train, missing_mask)\n",
        "        x_test_missing_mask = self._create_missing_mask(x_test) \n",
        "        x_test = self.fill(x_test, x_test_missing_mask)\n",
        "        observed_mask = ~missing_mask\n",
        "        x_test_observed_mask = ~x_test_missing_mask\n",
        "        for epoch in range(train_epochs):\n",
        "            X_pred = self._train_epoch(x_train, missing_mask, batch_size)\n",
        "            x_test_with_mask = np.hstack([x_test, x_test_missing_mask])\n",
        "            X_test_pred = self.predict(x_test_with_mask)\n",
        "            observed_mae = masked_mae(X_true=x_train,\n",
        "                                      X_pred=X_pred,\n",
        "                                      mask=observed_mask)\n",
        "            test_observed_mae = masked_mae(X_true=x_test,\n",
        "                                X_pred = X_test_pred,\n",
        "                                mask=x_test_observed_mask)\n",
        "            if epoch % 50 == 0:\n",
        "                print(\"Traing observed mae:\", observed_mae)\n",
        "                print(\"Test observed mae:\", test_observed_mae)\n",
        "            old_weight = (1.0 - self.recurrent_weight)\n",
        "            x_train[missing_mask] *= old_weight\n",
        "            x_test[x_test_missing_mask] *= old_weight\n",
        "            pred_missing = X_pred[missing_mask]\n",
        "            x_test_pred_missing = X_test_pred[x_test_missing_mask]\n",
        "            x_train[missing_mask] += self.recurrent_weight * pred_missing\n",
        "            x_test[x_test_missing_mask] += self.recurrent_weight*x_test_pred_missing\n",
        "        return x_train.copy(), x_test.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IAcuz3B0ohLD",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "seeds = [LOCAL_SEED+2, LOCAL_SEED+1, LOCAL_SEED+4, LOCAL_SEED+6, LOCAL_SEED+8]\n",
        "rmses = []\n",
        "cols = df_non_null.columns\n",
        "non_null_values = df_non_null.values.copy()\n",
        "for seed_number in seeds:\n",
        "    values = missing_encoded.values.copy()\n",
        "    train, test, comp_train, comp_test = train_test_split(values.copy(),\n",
        "                                                        non_null_values.copy(),\n",
        "                                                        test_size=0.1,\n",
        "                                                        random_state=seed_number)\n",
        "    df_test_complete = pd.DataFrame(columns=cols, \n",
        "                                    data=comp_test.copy())\n",
        "    scaler = MinMaxScaler().fit(train)\n",
        "    x_train = scaler.transform(train)\n",
        "    x_test = scaler.transform(test)\n",
        "    n_dims = x_train.shape[1]\n",
        "    aedropout = VAEDropout(n_dims=n_dims)\n",
        "    complete_encoded = aedropout.train(x_train.copy(), \n",
        "                                    x_test.copy(), \n",
        "                                    train_epochs=n_epochs,\n",
        "                                    batch_size=n_batch_size)\n",
        "    train_encoded, test_encoded = complete_encoded\n",
        "    missing_cols = list(missing_encoded)\n",
        "    inverse_test_encoded = scaler.inverse_transform(test_encoded)\n",
        "    df_test_dummies = pd.DataFrame(columns=missing_cols, \n",
        "                                   data=inverse_test_encoded)\n",
        "    df_test_dummies = reverse_encoding(df_test_dummies.copy())\n",
        "    df_test_dummies.drop(cat_cols, axis=1,\n",
        "                         inplace=True)\n",
        "    df_test_complete.drop(cat_cols, axis=1,\n",
        "                          inplace=True)\n",
        "    true_vals = df_test_complete.values.copy()\n",
        "    test_vals = df_test_dummies.values.copy()\n",
        "    scaler2 = MinMaxScaler().fit(true_vals)\n",
        "    scaled_true_vales = scaler2.transform(true_vals)\n",
        "    scaled_test_vales = scaler2.transform(test_vals)\n",
        "    rmse = math.sqrt(mean_squared_error(scaled_true_vales, \n",
        "                                        scaled_test_vales))\n",
        "    rmses.append(rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L6LzJCNcohLO",
        "colab": {}
      },
      "source": [
        "print(np.mean(rmses))\n",
        "print(np.std(rmses))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8y6VKVBMqyPB"
      },
      "source": [
        "# AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dcd8G7YAqyPX",
        "colab": {}
      },
      "source": [
        "class Autoencoder:\n",
        "    def __init__(self, n_dims,\n",
        "                 recurrent_weight=0.5,\n",
        "                 optimizer=\"adam\",\n",
        "                 dropout_probability=0.1,\n",
        "                 hidden_activation=\"relu\",\n",
        "                 output_activation=\"sigmoid\",\n",
        "                 init=\"glorot_normal\",\n",
        "                 l1_penalty=0,\n",
        "                 l2_penalty=1e-3,\n",
        "                 hidden_size=hidden_size):\n",
        "        self.n_dims = n_dims\n",
        "        self.recurrent_weight = recurrent_weight\n",
        "        self.optimizer = optimizer\n",
        "        self.dropout_probability = dropout_probability\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.output_activation = output_activation\n",
        "        self.init = init\n",
        "        self.l1_penalty = l1_penalty\n",
        "        self.l2_penalty = l2_penalty\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def make_reconstruction_loss(self, n_features):\n",
        "    \n",
        "        def reconstruction_loss(input_and_mask, y_pred):\n",
        "            X_values = input_and_mask[:, :n_features]\n",
        "            missing_mask = input_and_mask[:, n_features:]\n",
        "            observed_mask = 1 - missing_mask\n",
        "            X_values_observed = X_values * observed_mask\n",
        "            pred_observed = y_pred * observed_mask\n",
        "            return binary_crossentropy(y_true=X_values_observed, \n",
        "                                       y_pred=pred_observed)\n",
        "        return reconstruction_loss\n",
        "\n",
        "    def _create_model(self):\n",
        "        latent_dim = int(np.ceil(self.n_dims*0.5))\n",
        "        inputs = Input(shape=(2*n_dims, ), name='encoder_input')\n",
        "        x = inputs\n",
        "        x = Dense(self.hidden_size, activation=self.hidden_activation,\n",
        "                  init=self.init)(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        x = Dense(self.hidden_size//4, activation=self.hidden_activation,\n",
        "                  init=self.init)(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        encoded = Dense(latent_dim, name='encoding')(x)\n",
        "        self.encoder = Model(inputs, encoded, name='encoder')\n",
        "        latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "        x = latent_inputs\n",
        "        x = Dense(self.hidden_size//4, activation=self.hidden_activation,\n",
        "                  init=self.init)(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        x = Dense(self.hidden_size, activation=self.hidden_activation,\n",
        "                  init=self.init)(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        outputs = Dense(n_dims, activation=self.output_activation)(x)\n",
        "        self.decoder = Model(latent_inputs, \n",
        "                             outputs, \n",
        "                             name='decoder')\n",
        "        outputs = self.decoder(self.encoder(inputs))\n",
        "        self.model = Model(inputs, outputs, name='vae_mlp')\n",
        "        loss_function = self.make_reconstruction_loss(n_dims)\n",
        "        self.model.compile(optimizer=self.optimizer, \n",
        "                           loss=loss_function)\n",
        "\n",
        "    def fill(self, data, missing_mask):\n",
        "        data[missing_mask] = -1\n",
        "        return data\n",
        "\n",
        "    def _create_missing_mask(self, data):\n",
        "        if data.dtype != \"f\" and data.dtype != \"d\":\n",
        "            data = data.astype(float)\n",
        "\n",
        "        return np.isnan(data)\n",
        "\n",
        "    def _train_epoch(self, data, missing_mask, batch_size):\n",
        "        input_with_mask = np.hstack([data, missing_mask])\n",
        "        n_samples = len(input_with_mask)\n",
        "        n_batches = int(np.ceil(n_samples / batch_size))\n",
        "        indices = np.arange(n_samples)\n",
        "        np.random.shuffle(indices)\n",
        "        X_shuffled = input_with_mask[indices]\n",
        "        for batch_idx in range(n_batches):\n",
        "            batch_start = batch_idx * batch_size\n",
        "            batch_end = (batch_idx + 1) * batch_size\n",
        "            batch_data = X_shuffled[batch_start:batch_end, :]\n",
        "            self.model.train_on_batch(batch_data, batch_data)\n",
        "        return self.model.predict(input_with_mask)\n",
        "\n",
        "    def train(self, x_train, x_test, batch_size=256, train_epochs=100):\n",
        "        missing_mask = self._create_missing_mask(x_train)\n",
        "        x_train = self.fill(x_train, missing_mask)\n",
        "        x_test_missing_mask = self._create_missing_mask(x_test) \n",
        "        x_test = self.fill(x_test, x_test_missing_mask)\n",
        "        self._create_model()\n",
        "        self.encoder.summary()\n",
        "        self.decoder.summary()\n",
        "        self.model.summary()\n",
        "        observed_mask = ~missing_mask\n",
        "        x_test_observed_mask = ~x_test_missing_mask\n",
        "        input_with_mask = np.hstack([x_train, missing_mask])\n",
        "        for epoch in range(train_epochs):\n",
        "            X_pred = self._train_epoch(x_train, missing_mask, batch_size)\n",
        "            x_test_with_mask = np.hstack([x_test, x_test_missing_mask])\n",
        "            X_test_pred = self.model.predict(x_test_with_mask)\n",
        "            observed_mae = masked_mae(X_true=x_train,\n",
        "                                    X_pred=X_pred,\n",
        "                                    mask=observed_mask)\n",
        "            test_observed_mae = masked_mae(X_true=x_test,\n",
        "                                           X_pred = X_test_pred,\n",
        "                                           mask=x_test_observed_mask)\n",
        "            if epoch % 50 == 0:\n",
        "                print(\"observed mae:\", observed_mae)\n",
        "                print(\"Test mae:\", test_observed_mae)\n",
        "\n",
        "            old_weight = (1.0 - self.recurrent_weight)\n",
        "            x_train[missing_mask] *= old_weight\n",
        "            x_test[x_test_missing_mask] *= old_weight\n",
        "            pred_missing = X_pred[missing_mask]\n",
        "            x_test_pred_missing = X_test_pred[x_test_missing_mask]\n",
        "            x_train[missing_mask] += self.recurrent_weight * pred_missing\n",
        "            x_test[x_test_missing_mask] += self.recurrent_weight * x_test_pred_missing\n",
        "        return x_train.copy(), x_test.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7csKlgi8p3L-",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "seeds = [LOCAL_SEED+2, LOCAL_SEED+1, LOCAL_SEED+4, LOCAL_SEED+6, LOCAL_SEED+8]\n",
        "rmses = []\n",
        "cols = df_non_null.columns\n",
        "non_null_values = df_non_null.values.copy()\n",
        "for seed_number in seeds:\n",
        "    values = missing_encoded.values.copy()\n",
        "    train, test, comp_train, comp_test = train_test_split(values.copy(),\n",
        "                                                        non_null_values.copy(),\n",
        "                                                        test_size=0.1,\n",
        "                                                        random_state=seed_number)\n",
        "    df_test_complete = pd.DataFrame(columns=cols, \n",
        "                                    data=comp_test.copy())\n",
        "    scaler = MinMaxScaler().fit(train)\n",
        "    x_train = scaler.transform(train)\n",
        "    x_test = scaler.transform(test)\n",
        "    n_dims = x_train.shape[1]\n",
        "    aedropout = Autoencoder(n_dims=n_dims)\n",
        "    complete_encoded = aedropout.train(x_train.copy(), \n",
        "                                    x_test.copy(), \n",
        "                                    train_epochs=n_epochs,\n",
        "                                    batch_size=n_batch_size)\n",
        "    train_encoded, test_encoded = complete_encoded\n",
        "    missing_cols = list(missing_encoded)\n",
        "    inverse_test_encoded = scaler.inverse_transform(test_encoded)\n",
        "    df_test_dummies = pd.DataFrame(columns=missing_cols, \n",
        "                                   data=inverse_test_encoded)\n",
        "    df_test_dummies = reverse_encoding(df_test_dummies.copy())\n",
        "    df_test_dummies.drop(cat_cols, axis=1,\n",
        "                         inplace=True)\n",
        "    df_test_complete.drop(cat_cols, axis=1,\n",
        "                          inplace=True)\n",
        "    true_vals = df_test_complete.values.copy()\n",
        "    test_vals = df_test_dummies.values.copy()\n",
        "    scaler2 = MinMaxScaler().fit(true_vals)\n",
        "    scaled_true_vales = scaler2.transform(true_vals)\n",
        "    scaled_test_vales = scaler2.transform(test_vals)\n",
        "    rmse = math.sqrt(mean_squared_error(scaled_true_vales, \n",
        "                                        scaled_test_vales))\n",
        "    rmses.append(rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mp0pR9vM5ij8",
        "colab": {}
      },
      "source": [
        "print(np.mean(rmses))\n",
        "print(np.std(rmses))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PjJ4ap2drgQk"
      },
      "source": [
        "# VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZF_5mh_ergRf",
        "colab": {}
      },
      "source": [
        "class VAE:\n",
        "\n",
        "    def __init__(self, n_dims,\n",
        "                 recurrent_weight=0.5,\n",
        "                 optimizer=\"adam\",\n",
        "                 dropout_probability=0.1,\n",
        "                 hidden_activation=\"relu\",\n",
        "                 output_activation=\"sigmoid\",\n",
        "                 init=\"glorot_normal\",\n",
        "                 l2_penalty=1e-3):\n",
        "        self.n_dims = n_dims\n",
        "        self.recurrent_weight = recurrent_weight\n",
        "        self.optimizer = optimizer\n",
        "        self.dropout_probability = dropout_probability\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.output_activation = output_activation\n",
        "        self.init = init\n",
        "        self.l2_penalty = l2_penalty\n",
        "        self.hidden_size = hidden_size\n",
        "        self._create_model()\n",
        "\n",
        "    def make_reconstruction_loss(self, n_features, z_mean, z_log_var):\n",
        "        def reconstruction_loss(input_and_mask, y_pred):\n",
        "            X_values = input_and_mask[:, :n_features]\n",
        "            missing_mask = input_and_mask[:, n_features:]\n",
        "            observed_mask = 1 - missing_mask\n",
        "            X_values_observed = X_values * observed_mask\n",
        "            pred_observed = y_pred * observed_mask\n",
        "            reconstruction_loss = binary_crossentropy(y_true=X_values_observed, \n",
        "                                        y_pred=pred_observed)\n",
        "            reconstruction_loss*=n_features\n",
        "            kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "            kl_loss = K.sum(kl_loss, axis=-1)\n",
        "            kl_loss *= -0.5\n",
        "            vae_loss = K.mean(reconstruction_loss + kl_loss)\t\n",
        "            return vae_loss\n",
        "        return reconstruction_loss\n",
        "\n",
        "    def sampling(self, args):\n",
        "        \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
        "        # Arguments\n",
        "            args (tensor): mean and log of variance of Q(z|X)\n",
        "        # Returns\n",
        "            z (tensor): sampled latent vector\n",
        "        \"\"\"\n",
        "    \n",
        "        z_mean, z_log_var = args\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        dim = K.int_shape(z_mean)[1]\n",
        "        # by default, random_normal has mean = 0 and std = 1.0\n",
        "        epsilon = K.random_normal(shape=(batch, dim))\n",
        "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    def _create_model(self):\n",
        "        latent_dim = (int(np.ceil(self.n_dims*0.5)))\n",
        "        inputs = Input(shape=(2*self.n_dims, ), name='encoder_input')\n",
        "        x = inputs\n",
        "        x = Dense(self.hidden_size, activation=self.hidden_activation,\n",
        "                  init=self.init)(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        x = Dense(self.hidden_size//4, activation=self.hidden_activation,\n",
        "                  init=self.init)(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "        z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "        z = Lambda(self.sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "        self.encoder = Model(inputs, [z_mean, \n",
        "                                      z_log_var, z], name='encoder')\n",
        "        latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "        x = latent_inputs\n",
        "        x = Dense(self.hidden_size//4, activation=self.hidden_activation,\n",
        "                  init=self.init)(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        x = Dense(self.hidden_size, activation=self.hidden_activation,\n",
        "                  init=self.init)(x)\n",
        "        x = Dropout(self.dropout_probability)(x)\n",
        "        outputs = Dense(self.n_dims, activation=self.output_activation)(x)\n",
        "        self.decoder = Model(latent_inputs, \n",
        "                             outputs, \n",
        "                             name='decoder')\n",
        "        outputs = self.decoder(self.encoder(inputs)[2])\n",
        "        self.model = Model(inputs, outputs, \n",
        "                           name='vae_mlp')\n",
        "        reconstruction_loss = self.make_reconstruction_loss(n_dims, \n",
        "                                                       z_mean, \n",
        "                                                       z_log_var)\n",
        "        self.model.compile(optimizer=self.optimizer, \n",
        "                           loss=reconstruction_loss)\n",
        "\n",
        "    def fill(self, data, missing_mask):\n",
        "        data[missing_mask] = -1\n",
        "        return data\n",
        "\n",
        "    def _create_missing_mask(self, data):\n",
        "        if data.dtype != \"f\" and data.dtype != \"d\":\n",
        "            data = data.astype(float)\n",
        "        return np.isnan(data)\n",
        "\n",
        "    def _train_epoch(self, data, missing_mask, batch_size):\n",
        "        input_with_mask = np.hstack([data, missing_mask])\n",
        "        n_samples = len(input_with_mask)\n",
        "        n_batches = int(np.ceil(n_samples / batch_size))\n",
        "        indices = np.arange(n_samples)\n",
        "        np.random.shuffle(indices)\n",
        "        X_shuffled = input_with_mask[indices]\n",
        "        for batch_idx in range(n_batches):\n",
        "            batch_start = batch_idx * batch_size\n",
        "            batch_end = (batch_idx + 1) * batch_size\n",
        "            batch_data = X_shuffled[batch_start:batch_end, :]\n",
        "            self.model.train_on_batch(batch_data, batch_data)\n",
        "        return self.model.predict(input_with_mask)\n",
        "\n",
        "    def train(self, x_train, x_test, batch_size=256, train_epochs=100):\n",
        "        missing_mask = self._create_missing_mask(x_train)\n",
        "        x_train = self.fill(x_train, missing_mask)\n",
        "        x_test_missing_mask = self._create_missing_mask(x_test) \n",
        "        x_test = self.fill(x_test, x_test_missing_mask)\n",
        "        observed_mask = ~missing_mask\n",
        "        x_test_observed_mask = ~x_test_missing_mask\n",
        "        for epoch in range(train_epochs):\n",
        "            X_pred = self._train_epoch(x_train, missing_mask, batch_size)\n",
        "            x_test_with_mask = np.hstack([x_test, x_test_missing_mask])\n",
        "            X_test_pred = self.model.predict(x_test_with_mask)\n",
        "            observed_mae = masked_mae(X_true=x_train,\n",
        "                                      X_pred=X_pred,\n",
        "                                      mask=observed_mask)\n",
        "            test_observed_mae = masked_mae(X_true=x_test,\n",
        "                                X_pred = X_test_pred,\n",
        "                                mask=x_test_observed_mask)\n",
        "            if epoch % 50 == 0:\n",
        "                print(\"Traing observed mae:\", observed_mae)\n",
        "                print(\"Test observed mae:\", test_observed_mae)\n",
        "            old_weight = (1.0 - self.recurrent_weight)\n",
        "            x_train[missing_mask] *= old_weight\n",
        "            x_test[x_test_missing_mask] *= old_weight\n",
        "            pred_missing = X_pred[missing_mask]\n",
        "            x_test_pred_missing = X_test_pred[x_test_missing_mask]\n",
        "            x_train[missing_mask] += self.recurrent_weight * pred_missing\n",
        "            x_test[x_test_missing_mask] += self.recurrent_weight*x_test_pred_missing\n",
        "        return x_train.copy(), x_test.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TrA0fOz75r5c",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "seeds = [LOCAL_SEED+2, LOCAL_SEED+1, LOCAL_SEED+4, LOCAL_SEED+6, LOCAL_SEED+8]\n",
        "rmses = []\n",
        "cols = df_non_null.columns\n",
        "non_null_values = df_non_null.values.copy()\n",
        "for seed_number in seeds:\n",
        "    values = missing_encoded.values.copy()\n",
        "    train, test, comp_train, comp_test = train_test_split(values.copy(),\n",
        "                                                        non_null_values.copy(),\n",
        "                                                        test_size=0.1,\n",
        "                                                        random_state=seed_number)\n",
        "    df_test_complete = pd.DataFrame(columns=cols, \n",
        "                                    data=comp_test.copy())\n",
        "    scaler = MinMaxScaler().fit(train)\n",
        "    x_train = scaler.transform(train)\n",
        "    x_test = scaler.transform(test)\n",
        "    n_dims = x_train.shape[1]\n",
        "    aedropout = VAE(n_dims=n_dims)\n",
        "    complete_encoded = aedropout.train(x_train.copy(), \n",
        "                                    x_test.copy(), \n",
        "                                    train_epochs=n_epochs,\n",
        "                                    batch_size=n_batch_size)\n",
        "    train_encoded, test_encoded = complete_encoded\n",
        "    missing_cols = list(missing_encoded)\n",
        "    inverse_test_encoded = scaler.inverse_transform(test_encoded)\n",
        "    df_test_dummies = pd.DataFrame(columns=missing_cols, \n",
        "                                   data=inverse_test_encoded)\n",
        "    df_test_dummies = reverse_encoding(df_test_dummies.copy())\n",
        "    df_test_dummies.drop(cat_cols, axis=1,\n",
        "                         inplace=True)\n",
        "    df_test_complete.drop(cat_cols, axis=1,\n",
        "                          inplace=True)\n",
        "    true_vals = df_test_complete.values.copy()\n",
        "    test_vals = df_test_dummies.values.copy()\n",
        "    scaler2 = MinMaxScaler().fit(true_vals)\n",
        "    scaled_true_vales = scaler2.transform(true_vals)\n",
        "    scaled_test_vales = scaler2.transform(test_vals)\n",
        "    rmse = math.sqrt(mean_squared_error(scaled_true_vales, \n",
        "                                        scaled_test_vales))\n",
        "    rmses.append(rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qS-OmRoM5r5p",
        "colab": {}
      },
      "source": [
        "print(np.mean(rmses))\n",
        "print(np.std(rmses))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}